\begin{frame}{A Brief History of Word Vectors}
  %% Word vectors capture linguistic regularities in a simple manner.
  \begin{align*}
  king - man + woman \approx queen
  \end{align*}  
  %% No explanation was given in the original papers.
  \footnotesize{Somewhat suprisingly, many of these patterns can be represented a linear translations}---\cite{Mikolov13a} \\
  %% Levy showed that tranditional methods like count sparse contexts can perform equally well.
  \footnotesize{... traditional word similarities can perform just as well as neural embeddings.}---\cite{Levy14}
\end{frame}

\begin{frame}{Tasks}
  \begin{itemize}
  \item Semantic Relatedness
    \begin{itemize}
      \item Athens is to Greece as Berlin is to ?
      \end{itemize}
  \item Syntactic Relatedness
    \begin{itemize}
      \item Car is to Cars as Family is to ?
      \item 
      \end{itemize}
  \end{itemize}
  %% \begin{itemize}
  %% \item Semantic Relatedness \\
  %%   \quote{Athens is to Greece as Berlin is to ?}
  %% \item Syntactic Relatedness \\
  %% \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
